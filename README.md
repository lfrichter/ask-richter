# Ask Richter - Meu CV Interativo com IA

[![Node.js](https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white)](https://nodejs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-3178C6?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Express.js](https://img.shields.io/badge/Express.js-000000?style=for-the-badge&logo=express&logoColor=white)](https://expressjs.com/)
[![Next.js](https://img.shields.io/badge/Next.js-000000?style=for-the-badge&logo=nextdotjs&logoColor=white)](https://nextjs.org/)
[![React](https://img.shields.io/badge/React-61DAFB?style=for-the-badge&logo=react&logoColor=black)](https://reactjs.org/)
[![Ollama](https://img.shields.io/badge/Ollama-000000?style=for-the-badge&logo=ollama&logoColor=white)](https://ollama.com/)
[![Turborepo](https://img.shields.io/badge/Turborepo-EF4444?style=for-the-badge&logo=turborepo&logoColor=white)](https://turbo.build/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Vercel](https://img.shields.io/badge/Vercel-000000?style=for-the-badge&logo=vercel&logoColor=white)](https://vercel.com/)

Este projeto transforma um curr√≠culo tradicional em uma experi√™ncia de di√°logo. O **Ask Richter** √© um chatbot especialista na minha trajet√≥ria profissional, permitindo que recrutadores e l√≠deres t√©cnicos fa√ßam perguntas em linguagem natural e recebam respostas inteligentes e contextuais, baseadas nos meus dados profissionais consolidados.

Mais do que um portf√≥lio, √© uma ferramenta de marketing profissional e uma demonstra√ß√£o pr√°tica de compet√™ncias em arquitetura de software moderna, desenvolvimento full-stack e integra√ß√£o com IA.

## üöÄ Principais Features

- **Interface de Chat Conversacional:** Uma UI limpa e reativa para um di√°logo fluido.
- **Respostas Contextuais com RAG:** Utiliza a t√©cnica de Retrieval-Augmented Generation, buscando informa√ß√µes em um banco vetorial **FAISS** para gerar respostas precisas.
- **Suporte a LLMs Locais com Ollama:** Configurado por padr√£o para rodar com modelos de linguagem locais (ex: Llama 3), permitindo testes e uso sem custo e com total privacidade.
- **Arquitetura Full-Stack Moderna:** Backend em Node.js/Express e Frontend em Next.js, ambos com TypeScript.
- **Estrutura em Monorepo:** Organizado com Turborepo para um desenvolvimento integrado e eficiente.

## üõ†Ô∏è Stack Tecnol√≥gica

| Camada               | Tecnologia                          | Descri√ß√£o                                                                                                      |
| :------------------- | :---------------------------------- | :------------------------------------------------------------------------------------------------------------- |
| **Backend**          | Node.js, Express.js, TypeScript     | API RESTful respons√°vel pela l√≥gica RAG e comunica√ß√£o com o OpenRouter.                                        |
| **Frontend**         | Next.js, React, Tailwind CSS        | Interface de usu√°rio reativa, utilizando o Vercel AI SDK para a gest√£o do estado do chat.                      |
| **IA & Dados**       | FAISS, LangChain, OpenAI Embeddings | FAISS para o banco vetorial local, LangChain para orquestra√ß√£o de dados e OpenAI para a gera√ß√£o de embeddings. |
| **DevOps & Tooling** | Turborepo, Docker, Vercel, Render   | Monorepo para gest√£o do c√≥digo, Docker para conteineriza√ß√£o e deploy em plataformas otimizadas.                |

## üèóÔ∏è Arquitetura do Sistema

O fluxo de dados √© projetado para ser simples e desacoplado, garantindo uma comunica√ß√£o eficiente entre o usu√°rio e o servi√ßo de IA.

```mermaid
graph TD
    subgraph "Fase 1: Indexa√ß√£o (Offline)"
        A1(Fonte de Dados<br>CV.md, Projetos.md) --> A2(Chunking<br>Fragmenta√ß√£o por Se√ß√£o)
        A2 --> A3(Modelo de Embedding<br>text-embedding-3-small)
        A3 --> A4(Banco de Dados Vetorial<br>FAISS)
    end

    subgraph "Fase 2: Consulta (Online)"
        B1(Usu√°rio faz pergunta) --> B2(Frontend<br>Next.js)
        B2 --> B3(Backend<br>Node.js API)
        B3 --> B4(Modelo de Embedding)
        B4 -- "Vetor da pergunta" --> B5(Busca no Banco Vetorial)
        A4 -- "Chunks relevantes" --> B5
        B5 --> B6(Montagem do Prompt<br>Contexto + Pergunta)
        B6 --> B7(LLM via OpenRouter<br>Llama 3, Mistral, etc.)
        B7 --> B3
        B3 --> B2
    end
````

## ‚öôÔ∏è Rodando o Projeto Localmente

Para executar o projeto no seu ambiente de desenvolvimento, siga os passos abaixo.

#### 1\. Pr√©-requisitos

  - **Node.js:** Vers√£o LTS (recomenda-se `nvm`).
  - **npm:** Vers√£o compat√≠vel com Node.js.
  - **Ollama:** A aplica√ß√£o requer o [Ollama](https://ollama.com/) instalado e rodando localmente.
      - Ap√≥s instalar, puxe um modelo de linguagem no seu terminal:
        ```bash
        ollama pull llama3
        ```

#### 2\. Instala√ß√£o

Clone o reposit√≥rio e instale as depend√™ncias a partir da raiz do monorepo.

```bash
git clone [https://github.com/seu-usuario/ask-richter.git](https://github.com/seu-usuario/ask-richter.git)
cd ask-richter
npm install
```

#### 3\. Vari√°veis de Ambiente

Navegue at√© a pasta do backend (`apps/backend`) e crie o seu arquivo `.env` a partir do exemplo.

```bash
cd apps/backend
cp .env.example .env
```

Agora, edite o arquivo `.env` e adicione suas chaves de API.

```env
# Chave da OpenAI (APENAS para gerar os embeddings, n√£o para o chat)
OPENAI_API_KEY="sk-..."

# Provedor de IA padr√£o. Mude para 'huggingface' para usar a API externa.
AI_PROVIDER="ollama"

# URL da sua inst√¢ncia Ollama
OLLAMA_BASE_URL="http://localhost:11434"

# (Opcional) Chave da Hugging Face, se for usar
HUGGINGFACE_API_KEY="hf_..."
```

#### 4. Gera√ß√£o do Banco Vetorial

O processo de indexa√ß√£o pode ser executado de duas formas, dependendo do seu ambiente.

**Modo 1: Gera√ß√£o Apenas Local (para Desenvolvimento)**

Neste modo, o √≠ndice √© criado na sua m√°quina e usado diretamente pelo backend. √â ideal para rodar o projeto localmente sem depend√™ncias externas.

!["Store Faiss.index"](https://i.imgur.com/KRuhmbs.png)

1.  Certifique-se de que as vari√°veis de ambiente do Supabase (`SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`) **n√£o** estejam definidas no seu arquivo `apps/backend/.env`.
2.  Execute o comando de build a partir da **raiz do projeto**:
    ```bash
    npm run build-index --workspace=backend
    ```
O script criar√° o √≠ndice no diret√≥rio `/tmp/faiss_index` e exibir√° um erro ao tentar fazer o upload para o Supabase, o que √© esperado. O servidor local usar√° esse √≠ndice automaticamente.

**Modo 2: Gera√ß√£o Local com Upload para o Supabase (para Produ√ß√£o/Deploy)**

Este modo √© usado para gerar o √≠ndice e envi√°-lo para um armazenamento persistente (Supabase Storage), de onde ele pode ser baixado por um ambiente de produ√ß√£o (ex: Render, Vercel).

!["Faiss.index in the Supabase"](https://i.imgur.com/h7KmEJY.png)

1.  Adicione as vari√°veis `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY` e `SUPABASE_BUCKET_NAME` ao seu arquivo `apps/backend/.env`.
2.  Execute o mesmo comando:
    ```bash
    npm run build-index --workspace=backend
    ```
O script criar√° o √≠ndice localmente e, em seguida, far√° o upload dos arquivos para o seu bucket no Supabase, tornando-os dispon√≠veis para download em ambientes de produ√ß√£o.

#### 5\. Execu√ß√£o

Com tudo configurado, inicie o ambiente de desenvolvimento a partir da **raiz do projeto**:

```bash
npm run dev
```

O Turborepo iniciar√° os dois servi√ßos:

  - O frontend estar√° dispon√≠vel em `http://localhost:3000`.
  - O backend estar√° dispon√≠vel em `http://localhost:3001`.

## ü§ù Como Contribuir

Este √© um projeto pessoal, mas estou aberto a sugest√µes e melhorias. Para garantir a qualidade e a consist√™ncia do c√≥digo, por favor, siga as diretrizes detalhadas no nosso **[Guia de Contribui√ß√£o](https://www.google.com/search?q=CONTRIBUTING.md)**.

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.