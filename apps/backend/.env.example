# ------------------------------------------------
# ARQUIVO DE EXEMPLO PARA O BACKEND (.env)
# ------------------------------------------------

# Chave da OpenAI (obrigatória para gerar os embeddings do índice FAISS)
OPENAI_API_KEY="sk-..."

# --- Configuração do Provedor de IA ---
# Escolha o provedor a ser usado: 'ollama' (local) ou 'huggingface' (nuvem)
AI_PROVIDER="ollama"

# --- Modelos ---
# Nome do modelo a ser usado pelo Ollama (ex: 'llama3', 'mistral:7b')
OLLAMA_MODEL="mistral:7b"
# Nome do modelo a ser usado pela Hugging Face
HUGGINGFACE_MODEL="meta-llama/Llama-3.1-8B-Instruct:novita"

# --- Chaves e URLs de Serviços ---
# URL da sua instância Ollama (geralmente não muda)
OLLAMA_BASE_URL="http://localhost:11434"

# Chave da Hugging Face (necessária se AI_PROVIDER for 'huggingface')
HUGGINGFACE_API_KEY="hf_..."

# Credenciais do Supabase (obrigatórias para o armazenamento do índice FAISS)
SUPABASE_URL="https://[SEU-ID-DO-PROJETO].supabase.co"
SUPABASE_SERVICE_ROLE_KEY="sua-chave-service-role-secreta-aqui"
SUPABASE_BUCKET_NAME="faiss-index"
